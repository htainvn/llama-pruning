model_name: "meditsolutions/Llama-3.2-SUN-1B-chat"
prune_percent: 0.2
dtype: "float32"
cache_dir: null
device: "cuda"
output: "results/pruned_model"
apply_chat_template: true
prompt: "What is the capital of France?"
max_new_tokens: 50
target_size: null
log_dir: "logs"
test_only: false
prune_method: "mk"
use_normalized_weights: false
use_layer_norm_tweaks: false
layer_norm_scale: 4.0
gate_up_weight_weights: [1.0, 1.0]
print_summary: false
quiet: false
stop_logging: false

# Prune grid search parameters
grid_search:
  prune_percent_list: [0.1, 0.2, 0.3]
  prune_method_list: ["mk_prune", "mk_prune_adjusted"]
  use_normalized_weights_list: [true, false]
  use_layer_norm_tweaks_list: [true, false]
  layer_norm_scale_list: [2.0, 4.0]
  target_size_list: [null, 512]
  gate_up_weight_weights_list: [[1.0, 1.0], [0.4, 0.6]]